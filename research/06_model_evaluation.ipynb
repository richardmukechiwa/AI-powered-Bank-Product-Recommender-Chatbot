{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a42ec141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\RICH-FILES\\\\Desktop\\\\ml\\\\AI-powered-Bank-Product-Recommender-Chatbot'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc54f688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\RICH-FILES\\\\Desktop\\\\ml'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0430e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"C:/Users/RICH-FILES/Desktop/ml/AI-powered-Bank-Product-Recommender-Chatbot\"\n",
    "os.chdir(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    metric_file_name: Path\n",
    "    target_column: str\n",
    "    params: dict[str, str]\n",
    "    \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BankProducts.constants import *\n",
    "from BankProducts.utils.common  import read_yaml, create_directories\n",
    "from BankProducts   import logger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation   \n",
    "        params = self.params.random_forest\n",
    "        schema =  self.schema.target_column\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            test_data_path=Path(config.test_data_path),\n",
    "            model_path=Path(config.model_path),\n",
    "            metric_file_name=Path(config.metric_file_name),\n",
    "            target_column=schema.name,\n",
    "            params=params\n",
    "            \n",
    "           \n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e720637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "from BankProducts import logger\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import tempfile\n",
    "import mlflow\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from BankProducts.utils.common import save_json\n",
    "import numpy as np\n",
    "import shap\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='_distutils_hack')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def eval_metrics(self, actual, pred):\n",
    "        accuracy = accuracy_score(actual, pred)\n",
    "        precision = precision_score(actual, pred, average='weighted')\n",
    "        recall = recall_score(actual, pred, average='weighted')\n",
    "        f1 = f1_score(actual, pred, average='weighted')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def log_confusion_matrix(self, actual, predicted, class_names):\n",
    "        cm = confusion_matrix(actual, predicted)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        temp_img_path = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False).name\n",
    "        plt.savefig(temp_img_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(temp_img_path, artifact_path=\"confusion_matrix\")\n",
    "\n",
    "    def log_classification_report(self, actual, predicted, class_names):\n",
    "        report = classification_report(actual, predicted, target_names=class_names)\n",
    "        temp_txt_path = tempfile.NamedTemporaryFile(suffix=\".txt\", delete=False).name\n",
    "        with open(temp_txt_path, \"w\") as f:\n",
    "            f.write(report)\n",
    "        mlflow.log_artifact(temp_txt_path, artifact_path=\"bank_products_recommender\")\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "        test_x = test_data.drop(columns=[self.config.target_column])\n",
    "        test_y = test_data[self.config.target_column]\n",
    "\n",
    "        # Encode the target variable\n",
    "        le = LabelEncoder()\n",
    "        test_y_encoded = le.fit_transform(test_y)\n",
    "\n",
    "        logger.info(\"Loading model from path: %s\", self.config.model_path)\n",
    "        pipeline = joblib.load(self.config.model_path)\n",
    "\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        mlflow.set_experiment(\"Product Recommender\")\n",
    "\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run()\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            predicted = pipeline.predict(test_x)\n",
    "\n",
    "            accuracy, precision, recall, f1 = self.eval_metrics(test_y_encoded, predicted)\n",
    "\n",
    "            scores = {\n",
    "                \"model_name\": \"random_classifier\",\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1\n",
    "            }\n",
    "            \n",
    "            # Ensure directory exists\n",
    "            Path(self.config.metric_file_name).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            save_json(Path(self.config.metric_file_name), data=scores)\n",
    "            \n",
    "            logger.info(\"Metrics saved to: %s\", self.config.metric_file_name)\n",
    "\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"precision\", precision)\n",
    "            mlflow.log_metric(\"recall\", recall)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "            class_names = le.classes_\n",
    "            self.log_confusion_matrix(test_y_encoded, predicted, class_names)\n",
    "            self.log_classification_report(test_y_encoded, predicted, class_names)\n",
    "\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.sklearn.log_model(pipeline, \"pipeline\", registered_model_name=\"product recommender\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(pipeline, \"pipeline\")\n",
    "              \n",
    "            logger.info(\"mlflow model logged successfully.\")\n",
    " \n",
    "    def feature_importance(self):\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "        test_x = test_data.drop(columns=[self.config.target_column])\n",
    "        test_y = test_data[self.config.target_column]\n",
    "\n",
    "        pipeline = joblib.load(self.config.model_path)\n",
    "\n",
    "        \n",
    "        preprocessor = pipeline.named_steps['preprocessor']\n",
    "        model = pipeline.named_steps['classifier']\n",
    "        X_processed = preprocessor.transform(test_x)\n",
    "\n",
    "        try:\n",
    "            feature_names = preprocessor.get_feature_names_out()\n",
    "        except AttributeError:\n",
    "            num_features = preprocessor.transformers_[0][2]\n",
    "            cat_encoder = preprocessor.transformers_[1][1]\n",
    "            cat_features = cat_encoder.get_feature_names_out(preprocessor.transformers_[1][2])\n",
    "            feature_names = np.concatenate([num_features, cat_features])\n",
    "\n",
    "        X_df = pd.DataFrame(\n",
    "            X_processed.toarray() if hasattr(X_processed, 'toarray') else X_processed,\n",
    "            columns=feature_names\n",
    "        )\n",
    "\n",
    "        logger.info(\"Generating SHAP values...\")\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_df)\n",
    "\n",
    "        shap.summary_plot(shap_values, X_df, show=False)\n",
    "\n",
    "        # Mean absolute SHAP values\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_array = np.abs(np.array(shap_values)).mean(axis=0)\n",
    "        else:\n",
    "            shap_array = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "        feature_importance = pd.DataFrame(\n",
    "            list(zip(X_df.columns, shap_array)),\n",
    "            columns=['Feature', 'Mean Absolute SHAP Value']\n",
    "        ).sort_values(by='Mean Absolute SHAP Value', ascending=False)\n",
    "\n",
    "        logger.info(\"Feature importance:\\n%s\", feature_importance)\n",
    "\n",
    "        # Select top N features\n",
    "        n_top_features = 5\n",
    "        selected_features = feature_importance['Feature'].head(n_top_features).tolist()\n",
    "\n",
    "        X_selected = test_x[selected_features]\n",
    "\n",
    "        # Re-encode target variable\n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(test_y)\n",
    "\n",
    "        # Train on selected features\n",
    "        model_selected = RandomForestClassifier(random_state=42)\n",
    "        model_selected.fit(X_selected, y_encoded)\n",
    "\n",
    "        y_pred_selected = model_selected.predict(X_selected)\n",
    "\n",
    "        # MAPE (for classification, not common, but preserved from original)\n",
    "        def mape(y_true, y_pred):\n",
    "            y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "            return np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1)))\n",
    "\n",
    "        mape_value = mape(y_encoded, y_pred_selected) * 100\n",
    "        print(f\"MAPE with selected features: {mape_value:.2f} %\")\n",
    "\n",
    "        logger.info(\"Top important features selected using SHAP.\")\n",
    "        return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 14:13:35,578: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-26 14:13:35,597: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-26 14:13:35,622: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-05-26 14:13:35,624: INFO: common: created directory at: artifacts]\n",
      "[2025-05-26 14:13:35,625: INFO: common: created directory at: artifacts]\n",
      "[2025-05-26 14:13:35,637: INFO: 1283244525: Loading model from path: artifacts\\model_training\\model.joblib]\n",
      "[2025-05-26 14:13:36,056: INFO: common: json file saved at: artifacts\\model_evaluation\\metrics.json]\n",
      "[2025-05-26 14:13:36,057: INFO: 1283244525: Metrics saved to: artifacts\\model_evaluation\\metrics.json]\n",
      "An error occurred: No module named 'distutils._modified'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    eval = ModelEvaluation(config=model_evaluation_config)\n",
    "    eval.log_into_mlflow()\n",
    "    #eval.feature_importance()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9469ccb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710aab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19386016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa7a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8dca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55592b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40b527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06341f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bankprod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
